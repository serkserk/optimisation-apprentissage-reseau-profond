{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random as ran\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "#plt.style.available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAFFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdata = \"jaffe\"\n",
    "\n",
    "jaffe_X = pd.read_csv(\"jaffeX.csv\",delimiter=\";\")\n",
    "jaffe_y = pd.read_csv(\"jaffeY.csv\",delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaffe_y = pd.get_dummies(jaffe_y.values.reshape([jaffe_y.shape[1]])).values +0.\n",
    "jaffe_X = jaffe_X.values/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'un exemple d'image\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    img = jaffe_X[i].reshape([26,26])\n",
    "    plt.imshow(img.transpose(),cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM SPLIT JAFFE DATASET\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( jaffe_X, jaffe_y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Number of samples in training set : \",X_train.shape[0])\n",
    "print(\"Number of samples in test set : \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#currentdata = \"mnist\"\n",
    "\n",
    "#mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-b50a9b0e52ff>:3: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data.\n",
      "WARNING:tensorflow:From c:\\users\\serks\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\serks\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\serks\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\serks\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\serks\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\serks\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "currentdata = \"mnist\"\n",
    "\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "eval_data = mnist.test.images\n",
    "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in training set :  41250\n",
      "Number of samples in test set :  13750\n"
     ]
    }
   ],
   "source": [
    "# RANDOM SPLIT MNIST DATASET\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( train_data, train_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Number of samples in training set : \",X_train.shape[0])\n",
    "print(\"Number of samples in test set : \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network with Random initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION DU RESEAU DE NEURONE\n",
    "\n",
    "sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 676])\n",
    "\n",
    "# XAVIER INITIALIZER\n",
    "initializer =  tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# NORMAL DISTRIBUTION\n",
    "#initializer = tf.random_normal_initializer()\n",
    "\n",
    "# ZEROS INITIALIZER\n",
    "#initializer = tf.zeros\n",
    "\n",
    "\n",
    "W1 = tf.Variable(initializer([676,100]))\n",
    "b1 = tf.Variable(initializer([100]))\n",
    "\n",
    "y1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
    "\n",
    "W2 = tf.Variable(initializer([100,50]))\n",
    "b2 = tf.Variable(initializer([50]))\n",
    "\n",
    "y2 = tf.nn.sigmoid(tf.add(tf.matmul(y1,W2),b2))\n",
    "\n",
    "W3 = tf.Variable(initializer([50,10]))\n",
    "b3 = tf.Variable(initializer([10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.add(tf.matmul(y2,W3),b3))\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), axis=1))\n",
    "\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING : \n",
    "\n",
    "acs = []\n",
    "ls = []\n",
    "\n",
    "number_of_epoch = 1000\n",
    "\n",
    "for i in range(number_of_epoch):\n",
    "    ac = sess.run(accuracy,feed_dict={x : X_test, y_ : y_test})\n",
    "    l = sess.run(cross_entropy,feed_dict={x : X_train, y_ : y_train})\n",
    "    sess.run(training,feed_dict={x : X_train, y_ : y_train})\n",
    "    acs.append(ac)\n",
    "    ls.append(l)\n",
    "    if(i%10 == 0):\n",
    "        print(\"epoch : \", i, \"/\", number_of_epoch)\n",
    "        print(\"accuracy : \" , ac)\n",
    "        print(\"loss : \",l )\n",
    "        print()\n",
    "        \n",
    "plt.plot(acs)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init1 = tf.contrib.layers.xavier_initializer()\n",
    "init2 = tf.random_normal_initializer()\n",
    "init3 = tf.zeros\n",
    "inits = [init1, init2, init3]\n",
    "\n",
    "acsList = []\n",
    "lsList = []\n",
    "\n",
    "initsS = [\"xavier\", \"distribution normal\", \"zéro\"]\n",
    "\n",
    "for initializer, title in zip(inits, initsS):\n",
    "\n",
    "    print(title)\n",
    "    \n",
    "    try:\n",
    "        sess.close()\n",
    "    except NameError:\n",
    "        sess = None\n",
    "    # CREATION DU RESEAU DE NEURONE\n",
    "    sess = tf.Session()\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 676])\n",
    "\n",
    "    # XAVIER INITIALIZER\n",
    "    #initializer =  tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    # NORMAL DISTRIBUTION\n",
    "    #initializer = tf.random_normal_initializer()\n",
    "\n",
    "    # ZEROS INITIALIZER\n",
    "    #initializer = tf.zeros\n",
    "\n",
    "    W1 = tf.Variable(initializer([676,100]))\n",
    "    b1 = tf.Variable(initializer([100]))\n",
    "\n",
    "    y1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
    "\n",
    "    W2 = tf.Variable(initializer([100,50]))\n",
    "    b2 = tf.Variable(initializer([50]))\n",
    "\n",
    "    y2 = tf.nn.sigmoid(tf.add(tf.matmul(y1,W2),b2))\n",
    "\n",
    "    W3 = tf.Variable(initializer([50,10]))\n",
    "    b3 = tf.Variable(initializer([10]))\n",
    "\n",
    "    y = tf.nn.softmax(tf.add(tf.matmul(y2,W3),b3))\n",
    "\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "    cross_entropy = -tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), axis=1))\n",
    "\n",
    "    LEARNING_RATE = 0.1\n",
    "\n",
    "    training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    # TRAINING : \n",
    "\n",
    "    acs = []\n",
    "    ls = []\n",
    "\n",
    "    number_of_epoch = 1000\n",
    "\n",
    "    for i in range(number_of_epoch):\n",
    "        ac = sess.run(accuracy,feed_dict={x : X_test, y_ : y_test})\n",
    "        l = sess.run(cross_entropy,feed_dict={x : X_train, y_ : y_train})\n",
    "        sess.run(training,feed_dict={x : X_train, y_ : y_train})\n",
    "        acs.append(ac)\n",
    "        ls.append(l)\n",
    "        if(i%10 == 0):\n",
    "            print(\"epoch : \", i, \"/\", number_of_epoch, end=\", \")\n",
    "            print(\"accuracy : \" , ac, end=\", \")\n",
    "            print(\"loss : \",l )\n",
    "    acsList.append(acs)\n",
    "    lsList.append(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for acs, ls, title in zip(acsList, lsList, initsS):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    ax = plt.subplot(121)    \n",
    "    ax.spines[\"top\"].set_visible(False)    \n",
    "    ax.spines[\"bottom\"].set_visible(False)    \n",
    "    ax.spines[\"right\"].set_visible(False)    \n",
    "    ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "    ax.get_xaxis().tick_bottom()    \n",
    "    ax.get_yaxis().tick_left()\n",
    "\n",
    "    plt.plot(acs)\n",
    "\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.title(\"evolution de la précision en fonction du nombre d'epoch\")\n",
    "\n",
    "    ax = plt.subplot(122)    \n",
    "    ax.spines[\"top\"].set_visible(False)    \n",
    "    ax.spines[\"bottom\"].set_visible(False)    \n",
    "    ax.spines[\"right\"].set_visible(False)    \n",
    "    ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "    ax.get_xaxis().tick_bottom()    \n",
    "    ax.get_yaxis().tick_left()\n",
    "\n",
    "    plt.plot(ls)\n",
    "\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"evolution du loss en fonction du nombre d'epoch\")\n",
    "\n",
    "    plt.suptitle(\"Initialisation {}\".format(title), size=16)\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTO ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 676])\n",
    "\n",
    "# encoder layers : \n",
    "\n",
    "w1 = tf.Variable(initializer([676, 100]))\n",
    "b1 = tf.Variable(initializer([100]))\n",
    "\n",
    "layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(X, w1),b1))\n",
    "\n",
    "w2 = tf.Variable(initializer([100, 50]))\n",
    "b2 = tf.Variable(initializer([50]))\n",
    "\n",
    "layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w2),b2))\n",
    "\n",
    "# decoder layers : \n",
    "\n",
    "w3 = tf.Variable(initializer([50, 100]))\n",
    "b3 = tf.Variable(initializer([100]))\n",
    "\n",
    "layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, w3),b3))\n",
    "\n",
    "w4 = tf.Variable(initializer([100, 676]))\n",
    "b4 = tf.Variable(initializer([676]))\n",
    "\n",
    "y_pred = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, w4),b4))\n",
    "\n",
    "y_true = X\n",
    "\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    sess.run(optimizer,feed_dict={X : X_train})\n",
    "    l = sess.run(loss,feed_dict={X : X_train} )\n",
    "    if i % 10 == 0 : \n",
    "        print(\"loss : \", l)\n",
    "\n",
    "img_reconstructed = sess.run(y_pred, feed_dict={X : X_test})\n",
    "\n",
    "# SHOW 10 reconstructed images :\n",
    "\n",
    "for i in range(10):\n",
    "    image = img_reconstructed[i].reshape([26,26])\n",
    "    plt.imshow(image.transpose(),cmap = \"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with autoencoder initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION DU RESEAU DE NEURONE\n",
    "sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 676])\n",
    "\n",
    "# XAVIER INITIALIZER\n",
    "initializer =  tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "W1 = w1\n",
    "B1 = b1\n",
    "\n",
    "y1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1),B1))\n",
    "\n",
    "W2 = w2\n",
    "B2 = b2\n",
    "\n",
    "y2 = tf.nn.sigmoid(tf.add(tf.matmul(y1,W2),B2))\n",
    "\n",
    "W3 = tf.Variable(initializer([50,10]))\n",
    "B3 = tf.Variable(initializer([10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.add(tf.matmul(y2,W3),B3))\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), axis=1))\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING : \n",
    "\n",
    "acs_AE = []\n",
    "ls_AE = []\n",
    "\n",
    "number_of_epoch = 1000\n",
    "\n",
    "for i in range(number_of_epoch):\n",
    "    ac = sess.run(accuracy,feed_dict={x : X_test, y_ : y_test})\n",
    "    l = sess.run(cross_entropy,feed_dict={x : X_train, y_ : y_train})\n",
    "    sess.run(training,feed_dict={x : X_train, y_ : y_train})\n",
    "    acs_AE.append(ac)\n",
    "    ls_AE.append(l)\n",
    "    if(i%10 == 0):\n",
    "        print(\"epoch : \", i, \"/\", number_of_epoch, end=\", \")\n",
    "        print(\"accuracy : \" , ac, end=\", \")\n",
    "        print(\"loss : \",l )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax = plt.subplot(121)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "plt.plot(acs_AE)\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"evolution de la précision en fonction du nombre d'epoch\")\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "plt.plot(ls_AE)\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"evolution du loss en fonction du nombre d'epoch\")\n",
    "\n",
    "plt.suptitle(\"Initialisation avec auto encoder\", size=16)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA initialisation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fir\n",
    "\n",
    "data = X_train.copy()\n",
    "\n",
    "m = data.mean(axis = 0 )\n",
    "#data -= data.mean(axis = 0 )\n",
    "cov = np.cov(data,rowvar=False)\n",
    "evals , evecs = np.linalg.eigh(cov)\n",
    "\n",
    "idx = np.argsort(evals)[::-1]\n",
    "R = evecs[:,idx]\n",
    "inertia = evals[idx]\n",
    "\n",
    "Rm =np.dot(m,R)\n",
    "respca = np.dot(data, R) - Rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.dot(respca,np.linalg.inv(R) + m )[100].reshape([26,26]).T,cmap = \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second PCA\n",
    "\n",
    "data = respca.copy()\n",
    "m2 = data.mean(axis = 0 )\n",
    "\n",
    "cov = np.cov(data,rowvar=False)\n",
    "evals , evecs = np.linalg.eigh(cov)\n",
    "\n",
    "idx = np.argsort(evals)[::-1]\n",
    "R2 = evecs[:,idx]\n",
    "inertia = evals[idx]\n",
    "\n",
    "Rm2 =np.dot(m,R)\n",
    "respca = np.dot(data, R) - Rm2\n",
    "\n",
    "reconstructed = np.linalg.inv(R) + m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = (np.dot(np.linalg.inv(R2),reconstructed) + m)[126].reshape([26,26])\n",
    "plt.imshow(one.T,cmap =\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[25].reshape([26,26]).T,cmap= \"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO FLOAT32\n",
    "\n",
    "R = np.array(R,dtype=\"float32\")[:,:100]\n",
    "R2 = np.array(R2,dtype=\"float32\")[:100,:50]\n",
    "Rm = np.array(Rm,dtype=\"float32\")[:100]\n",
    "Rm2 = np.array(Rm2,dtype=\"float32\")[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALISATION WITH PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATION DU RESEAU DE NEURONE\n",
    "sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 676])\n",
    "\n",
    "# XAVIER INITIALIZER\n",
    "initializer =  tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "W1 = tf.Variable(R)\n",
    "B1 = tf.Variable((-Rm))\n",
    "\n",
    "y1 = tf.nn.sigmoid(tf.add(tf.matmul(x,W1),B1))\n",
    "\n",
    "W2 = tf.Variable(R2)\n",
    "B2 = tf.Variable(-Rm2)\n",
    "\n",
    "y2 = tf.nn.sigmoid(tf.add(tf.matmul(y1,W2),B2))\n",
    "\n",
    "W3 = tf.Variable(initializer([50,10]))\n",
    "B3 = tf.Variable(initializer([10]))\n",
    "\n",
    "y = tf.nn.softmax(tf.add(tf.matmul(y2,W3),B3))\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "cross_entropy = -tf.reduce_mean(tf.reduce_sum(y_ * tf.log(y) + (1 - y_) * tf.log(1 - y), axis=1))\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "training = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING : \n",
    "\n",
    "acs_PCA = []\n",
    "ls_PCA = []\n",
    "\n",
    "number_of_epoch = 1000\n",
    "\n",
    "for i in range(number_of_epoch):\n",
    "    ac = sess.run(accuracy,feed_dict={x : X_test, y_ : y_test})\n",
    "    l = sess.run(cross_entropy,feed_dict={x : X_train, y_ : y_train})\n",
    "    sess.run(training,feed_dict={x : X_train, y_ : y_train})\n",
    "    acs_PCA.append(ac)\n",
    "    ls_PCA.append(l)\n",
    "    if(i%10 == 0):\n",
    "        print(\"epoch : \", i, \"/\", number_of_epoch, end=', ')\n",
    "        print(\"accuracy : \" , ac, end=', ')\n",
    "        print(\"loss : \",l )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax = plt.subplot(121)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "plt.plot(acs_PCA)\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"evolution de la précision en fonction du nombre d'epoch\")\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "plt.plot(ls_PCA)\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"evolution du loss en fonction du nombre d'epoch\")\n",
    "\n",
    "plt.suptitle(\"Initialisation aléatoire\", size=16)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot poster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "ax = plt.subplot(121)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(acsList[0], label=\"aleatoire xavier\")\n",
    "plt.plot(acsList[1], label=\"aleatoire distrib normal\")\n",
    "plt.plot(acsList[2], label=\"zéro\")\n",
    "plt.plot(acs_AE, label=\"auto encoder\")\n",
    "plt.plot(acs_PCA, label=\"pca\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"evolution de la précision en fonction du nombre d'epoch\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "ax = plt.subplot(122)    \n",
    "ax.spines[\"top\"].set_visible(False)    \n",
    "ax.spines[\"bottom\"].set_visible(False)    \n",
    "ax.spines[\"right\"].set_visible(False)    \n",
    "ax.spines[\"left\"].set_visible(False) \n",
    "\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()\n",
    "\n",
    "plt.plot(lsList[0], label=\"aleatoire xavier\")\n",
    "plt.plot(lsList[1], label=\"aleatoire distrib normal\")\n",
    "plt.plot(lsList[2], label=\"zéro\")\n",
    "plt.plot(ls_AE, label=\"auto encoder\")\n",
    "plt.plot(ls_PCA, label=\"pca\")\n",
    "\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"evolution du loss en fonction du nombre d'epoch\")\n",
    "plt.legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des initialisations\", size=16)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
